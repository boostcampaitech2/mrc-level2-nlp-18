{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# retrieval 코드 뜯어보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import faiss\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "from contextlib import contextmanager\n",
    "from typing import List, Tuple, NoReturn, Any, Optional, Union\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from datasets import (\n",
    "    Dataset,\n",
    "    load_from_disk,\n",
    "    concatenate_datasets,\n",
    ")\n",
    "\n",
    "class SparseRetrieval:\n",
    "    def __init__(\n",
    "        self,\n",
    "        tokenize_fn,\n",
    "        data_path: Optional[str] = \"../data/\",\n",
    "        context_path: Optional[str] = \"wikipedia_documents.json\",\n",
    "    ) -> NoReturn:\n",
    "\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            tokenize_fn:\n",
    "                기본 text를 tokenize해주는 함수입니다.\n",
    "                아래와 같은 함수들을 사용할 수 있습니다.\n",
    "                - lambda x: x.split(' ')\n",
    "                - Huggingface Tokenizer\n",
    "                - konlpy.tag의 Mecab\n",
    "\n",
    "            data_path:\n",
    "                데이터가 보관되어 있는 경로입니다.\n",
    "\n",
    "            context_path:\n",
    "                Passage들이 묶여있는 파일명입니다.\n",
    "\n",
    "            data_path/context_path가 존재해야합니다.\n",
    "\n",
    "        Summary:\n",
    "            Passage 파일을 불러오고 TfidfVectorizer를 선언하는 기능을 합니다.\n",
    "        \"\"\"\n",
    "\n",
    "        self.data_path = data_path\n",
    "        with open(os.path.join(data_path, context_path), \"r\", encoding=\"utf-8\") as f:\n",
    "            wiki = json.load(f)\n",
    "\n",
    "        self.contexts = list(\n",
    "            dict.fromkeys([v[\"text\"] for v in wiki.values()])\n",
    "        )  # set 은 매번 순서가 바뀌므로\n",
    "        print(f\"Lengths of unique contexts : {len(self.contexts)}\")\n",
    "        self.ids = list(range(len(self.contexts)))\n",
    "\n",
    "        # Transform by vectorizer\n",
    "        self.tfidfv = TfidfVectorizer(\n",
    "            tokenizer=tokenize_fn,\n",
    "            ngram_range=(1, 2),\n",
    "            max_features=50000,\n",
    "        )\n",
    "\n",
    "        self.p_embedding = None  # get_sparse_embedding()로 생성합니다\n",
    "        self.indexer = None  # build_faiss()로 생성합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path: Optional[str] = \"../data/\"\n",
    "context_path: Optional[str] = \"wikipedia_documents.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 기본 tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../data/wikipedia_documents.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-021414623e42>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mwiki\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mcontexts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfromkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"text\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mwiki\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# set은 순서가 바뀜\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Lengths of unique contexts : {len(contexts)}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../data/wikipedia_documents.json'"
     ]
    }
   ],
   "source": [
    "with open(os.path.join(data_path, context_path), \"r\", encoding=\"utf-8\") as f:\n",
    "    wiki = json.load(f)\n",
    "contexts = list(dict.fromkeys([v[\"text\"] for v in wiki.values()])) # set은 순서가 바뀜\n",
    "print(f\"Lengths of unique contexts : {len(contexts)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dataset_dict.json',\n",
       " 'validation',\n",
       " 'train',\n",
       " 'test_code.ipynb',\n",
       " 'sparse_embedding.bin',\n",
       " 'tfidv.bin']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "이 문서는 나라 목록이며, 전 세계 206개 나라의 각 현황과 주권 승인 정보를 개요 형태로 나열하고 있다.\n",
      "\n",
      "이 목록은 명료화를 위해 두 부분으로 나뉘어 있다.\n",
      "\n",
      "# 첫 번째 부분은 바티칸 시국과 팔레스타인을 포함하여 유엔 등 국제 기구에 가입되어 국제적인 승인을 널리 받았다고 여기는 195개 나라를 나열하고 있다.\n",
      "# 두 번째 부분은 일부 지역의 주권을 사실상 (데 팍토) 행사하고 있지만, 아직 국제적인 승인을 널리 받지 않았다고 여기는 11개 나라를 나열하고 있다.\n",
      "\n",
      "두 목록은 모두 가나다 순이다.\n",
      "\n",
      "일부 국가의 경우 국가로서의 자격에 논쟁의 여부가 있으며, 이 때문에 이러한 목록을 엮는 것은 매우 어렵고 논란이 생길 수 있는 과정이다. 이 목록을 구성하고 있는 국가를 선정하는 기준에 대한 정보는 \"포함 기준\" 단락을 통해 설명하였다. 나라에 대한 일반적인 정보는 \"국가\" 문서에서 설명하고 있다.\n"
     ]
    }
   ],
   "source": [
    "print(contexts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoConfig, AutoModelForQuestionAnswering, AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"seongju/klue-mrc-koelectra-base\",\n",
    "    use_fast=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PreTrainedTokenizerFast(name_or_path='seongju/klue-mrc-koelectra-base', vocab_size=35000, model_max_len=512, is_fast=True, padding_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method PreTrainedTokenizerFast.tokenize of PreTrainedTokenizerFast(name_or_path='seongju/klue-mrc-koelectra-base', vocab_size=35000, model_max_len=512, is_fast=True, padding_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'})>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TfidfVectorizer(max_features=50000, ngram_range=(1, 2),\n",
      "                tokenizer=<bound method PreTrainedTokenizerFast.tokenize of PreTrainedTokenizerFast(name_or_path='seongju/klue-mrc-koelectra-base', vocab_size=35000, model_max_len=512, is_fast=True, padding_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'})>)\n"
     ]
    }
   ],
   "source": [
    "tfidfv = TfidfVectorizer(\n",
    "    tokenizer=tokenizer.tokenize,\n",
    "    ngram_range=(1, 2),\n",
    "    max_features=50000,\n",
    ")\n",
    "print(tfidfv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get_sparse_embedding 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding pickle load.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Summary:\n",
    "    Passage Embedding을 만들고\n",
    "    TFIDF와 Embedding을 pickle로 저장합니다.\n",
    "    만약 미리 저장된 파일이 있으면 저장된 pickle을 불러옵니다.\n",
    "\"\"\"\n",
    "p_embedding = None  # get_sparse_embedding()로 생성합니다\n",
    "indexer = None  # build_faiss()로 생성합니다.\n",
    "# Pickle을 저장합니다.\n",
    "\n",
    "pickle_name = f\"sparse_embedding.bin\"\n",
    "tfidfv_name = f\"tfidv.bin\"\n",
    "emd_path = os.path.join(data_path, pickle_name)\n",
    "tfidfv_path = os.path.join(data_path, tfidfv_name)\n",
    "\n",
    "if os.path.isfile(emd_path) and os.path.isfile(tfidfv_path):\n",
    "    with open(emd_path, \"rb\") as file:\n",
    "        p_embedding = pickle.load(file)\n",
    "    with open(tfidfv_path, \"rb\") as file:\n",
    "        tfidfv = pickle.load(file)\n",
    "    print(\"Embedding pickle load.\")\n",
    "else:\n",
    "    print(\"Build passage embedding\")\n",
    "    p_embedding = tfidfv.fit_transform(contexts)\n",
    "    print(p_embedding.shape)\n",
    "    with open(emd_path, \"wb\") as file:\n",
    "        pickle.dump(p_embedding, file)\n",
    "    with open(tfidfv_path, \"wb\") as file:\n",
    "        pickle.dump(tfidfv, file)\n",
    "    print(\"Embedding pickle saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<56737x50000 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 18906074 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Context 갯수가 Row의 총 갯수\n",
    "# 그리고 기존에 max_features = 50,000 으로 두었기 때문에 이렇게 진행된다.\n",
    "p_embedding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
