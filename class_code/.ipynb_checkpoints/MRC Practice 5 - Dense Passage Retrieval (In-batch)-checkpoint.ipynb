{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WjDdziEN_VCt"
   },
   "source": [
    "# 5강) BERT를 활용한 Dense Passage Retrieval 실습"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1NWluWk3_VCu"
   },
   "source": [
    "### Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5421,
     "status": "ok",
     "timestamp": 1616574100645,
     "user": {
      "displayName": "Miyoung Ko",
      "photoUrl": "",
      "userId": "12757166311753740653"
     },
     "user_tz": -540
    },
    "id": "eGqFS4EEBF_Z",
    "outputId": "b5b5af1d-0d0d-4197-a717-d2fe3ca2528f"
   },
   "outputs": [],
   "source": [
    "# !pip install datasets\n",
    "# !pip install transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CYUkp06Y_VCv"
   },
   "source": [
    "## 데이터셋 로딩\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KMrZa4uql_nx"
   },
   "source": [
    "KorQuAD train 데이터셋을 학습 데이터로 활용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6098,
     "status": "ok",
     "timestamp": 1616574101330,
     "user": {
      "displayName": "Miyoung Ko",
      "photoUrl": "",
      "userId": "12757166311753740653"
     },
     "user_tz": -540
    },
    "id": "4IUxepuj_VCv",
    "outputId": "6c681d71-4e21-4062-9807-1d975fc901e8"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset squad_kor_v1 (/opt/ml/.cache/huggingface/datasets/squad_kor_v1/squad_kor_v1/1.0.0/31982418accc53b059af090befa81e68880acc667ca5405d30ce6fa7910950a7)\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"squad_kor_v1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lJtECqpB_VCx"
   },
   "source": [
    "## 토크나이저 준비 - Huggingface 제공 tokenizer 이용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X0Fu2WaqpUB8"
   },
   "source": [
    "BERT를 encoder로 사용하므로, hugginface에서 제공하는 \"bert-base-multilingual-cased\" tokenizer를 활용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "AoB8BHGDmVIK"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import numpy as np\n",
    "\n",
    "model_checkpoint = \"bert-base-multilingual-cased\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8395,
     "status": "ok",
     "timestamp": 1616574103635,
     "user": {
      "displayName": "Miyoung Ko",
      "photoUrl": "",
      "userId": "12757166311753740653"
     },
     "user_tz": -540
    },
    "id": "WPxRvMjdvh4y",
    "outputId": "069dd4b9-760a-450b-ea59-096510005e53"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PreTrainedTokenizerFast(name_or_path='bert-base-multilingual-cased', vocab_size=119547, model_max_len=512, is_fast=True, padding_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 120
    },
    "executionInfo": {
     "elapsed": 9195,
     "status": "ok",
     "timestamp": 1616574104440,
     "user": {
      "displayName": "Miyoung Ko",
      "photoUrl": "",
      "userId": "12757166311753740653"
     },
     "user_tz": -540
    },
    "id": "0U7sn3jsu44O",
    "outputId": "afb5de1c-c5d4-420c-d1f1-f69f56c44cd7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] 1839년 바그너는 괴테의 파우스트을 처음 읽고 그 내용에 마음이 끌려 이를 소재로 해서 하나의 교향곡을 쓰려는 뜻을 갖는다. 이 시기 바그너는 1838년에 빛 독촉으로 산전수전을 다 [UNK] 상황이라 좌절과 실망에 가득했으며 메피스토펠레스를 만나는 파우스트의 심경에 공감했다고 한다. 또한 파리에서 아브네크의 지휘로 파리 음악원 관현악단이 연주하는 베토벤의 교향곡 9번을 듣고 깊은 감명을 받았는데, 이것이 이듬해 1월에 파우스트의 서곡으로 쓰여진 이 작품에 조금이라도 영향을 끼쳤으리라는 것은 의심할 여지가 없다. 여기의 라단조 조성의 경우에도 그의 전기에 적혀 있는 것처럼 단순한 정신적 피로나 실의가 반영된 것이 아니라 베토벤의 합창교향곡 조성의 영향을 받은 것을 볼 수 있다. 그렇게 교향곡 작곡을 1839년부터 40년에 걸쳐 파리에서 착수했으나 1악장을 쓴 뒤에 중단했다. 또한 작품의 완성과 동시에 그는 이 서곡 ( 1악장 ) 을 파리 음악원의 연주회에서 연주할 파트보까지 준비하였으나, 실제로는 이루어지지는 않았다. 결국 초연은 4년 반이 지난 후에 드레스덴에서 연주되었고 재연도 이루어졌지만, 이후에 그대로 방치되고 말았다. 그 사이에 그는 리엔치와 방황하는 네덜란드인을 완성하고 탄호이저에도 착수하는 등 분주한 시간을 보냈는데, 그런 바쁜 생활이 이 곡을 잊게 한 것이 아닌가 하는 의견도 있다. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_input = tokenizer(dataset['train'][0]['context'], padding=\"max_length\", truncation=True)\n",
    "tokenizer.decode(tokenized_input['input_ids'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zpoTleVJjp5x"
   },
   "source": [
    "## Dense encoder (BERT) 학습 시키기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_nrxmtmfkRVb"
   },
   "source": [
    "HuggingFace BERT를 활용하여 question encoder, passage encoder 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "6b215ZfJ_EOc"
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm, trange\n",
    "import argparse\n",
    "import random\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import BertModel, BertPreTrainedModel, AdamW, TrainingArguments, get_linear_schedule_with_warmup\n",
    "\n",
    "torch.manual_seed(2021)\n",
    "torch.cuda.manual_seed(2021)\n",
    "np.random.seed(2021)\n",
    "random.seed(2021)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N-bKwkxTpoje"
   },
   "source": [
    "1) Training Dataset 준비하기 (question, passage pairs)\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "E_FQ1kcazxge"
   },
   "outputs": [],
   "source": [
    "# Use subset (128 example) of original training dataset \n",
    "sample_idx = np.random.choice(range(len(dataset['train'])), 128)\n",
    "training_dataset = dataset['train'][sample_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "NJZWx1b-613e"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import (DataLoader, RandomSampler, TensorDataset)\n",
    "\n",
    "q_seqs = tokenizer(training_dataset['question'], padding=\"max_length\", truncation=True, return_tensors='pt')\n",
    "p_seqs = tokenizer(training_dataset['context'], padding=\"max_length\", truncation=True, return_tensors='pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "bAplp66Pkayy"
   },
   "outputs": [],
   "source": [
    "train_dataset = TensorDataset(p_seqs['input_ids'], p_seqs['attention_mask'], p_seqs['token_type_ids'], \n",
    "                        q_seqs['input_ids'], q_seqs['attention_mask'], q_seqs['token_type_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([   101,   9294,  11102,  12092,  16617,  10892,   9998,  11513,   8863,\n",
       "         118816,  14843,  15891,  31398,   9448,  36210,  11287,   9708,  14646,\n",
       "          14523,  65096,  11018,   9926,  10622,   9116,  27792,  16139,    119,\n",
       "           9069,  37824,  15387,   9576,  11287,   9706,  33305,  20656,  17206,\n",
       "          44015,  64002,  13441,   9294,  11102,  12092,  16617,   9415,  14523,\n",
       "           9891,  38696,  11925,    119,   9318,  12605, 119304,  10622,   9845,\n",
       "          11664,   9583,  17342,  68828, 100158,  33378,   9102,  26737,  69592,\n",
       "          12178,  27487,  18589,  64002,  12945, 102574, 101472,   9641,  82888,\n",
       "           9641,  82888,   9326,  10530,  10459,   9812,  70162,  12030,   1886,\n",
       "           9368,  11287,  74986,  10892,  39218,   1887,   9590,  10020,  12945,\n",
       "          32679,  11018,   8996,  96404,  17706,    119,   9144,    117,  44757,\n",
       "         102574, 101472,  10622,   9934,  58931,  12609,  11018,   9637, 119438,\n",
       "          12092,   9322,  27792,  28750,    119,   8924,   9576,  11261,  22522,\n",
       "          16367,  37912,  64002,  13441,   9061,  28143,   9248,  27023,  12310,\n",
       "           9891,  38696,  10739,  23130,   9599,  22458,  27355,  12692,  10813,\n",
       "          10459, 102574, 101472,    107,   9952,  23811, 119249,  26444,    107,\n",
       "           9637,    100,  22522,  33323,  76854,  10459,   9061,  28143,   9640,\n",
       "            113,  10106,    114,   9344,  15184,  37004,    100,   8898,  10020,\n",
       "          12945,  32679,  11018,   9637, 119438,  11882,    117,  22522,  17461,\n",
       "          10530,  64002,  13441,   9069,  29683,   9559,  70162,  12508,   9891,\n",
       "          38696,  10459,   9651,  16617,  41521,   9323,  16617,  10739,   9049,\n",
       "         119439,  10813,  10459, 102574, 101472,  12030,    107,   9122,  28981,\n",
       "          16985,  17342,  14040,    107,   9637,    100,   9651,  16617,  41521,\n",
       "           9323,  16617,   9485,  86834,    100,   8898,  10020,  12945,  32679,\n",
       "          11018,   9637, 119438,  12092,  79602,  11506,    119,   8924,   9320,\n",
       "          35979,    100,   9137,  26344,   8932,  21611,  12638,   9283,  37388,\n",
       "          28000,   9730,  12310,    117,   9376,  56645,  10739,  20308,   8888,\n",
       "          13890,   8984,   9651,  16617,  41521,   9845,  12310,   9121,  10892,\n",
       "          44757,  10003,  12508,  10813,  12638,  22563,  10459, 102574, 101472,\n",
       "          10622,   9934,  58931,  12609,  11018,   9637, 119438,  10622,   9322,\n",
       "          27792,  28750,    119,    102,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0]),\n",
       " tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " tensor([   101,  20656,  10530,   9328,  30858,  16855,   9294,  11102,  12092,\n",
       "          16617,  10459,  64002,  10459,   9735,  25387,  53371,   9668,  23466,\n",
       "           9141,  12965,  32815,   8870,  53354,  11018,   9379,  33323,  10622,\n",
       "           9322, 119118,  23990,   9891,  38696,  10892,   9294, 119137,  12030,\n",
       "          11287,    136,    102,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0]),\n",
       " tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0]))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JwMvVH1e3h99"
   },
   "source": [
    "2) BERT encoder 학습시키기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vW7Oc7Zd9kkm"
   },
   "source": [
    "BertEncoder 모델 정의 후, question encoder, passage encoder에 pre-trained weight 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "oKKkTlh_l5VL"
   },
   "outputs": [],
   "source": [
    "class BertEncoder(BertPreTrainedModel):\n",
    "  def __init__(self, config):\n",
    "    super(BertEncoder, self).__init__(config)\n",
    "\n",
    "    self.bert = BertModel(config)\n",
    "    self.init_weights()\n",
    "      \n",
    "  def forward(self, input_ids, \n",
    "              attention_mask=None, token_type_ids=None): \n",
    "  \n",
    "      outputs = self.bert(input_ids,\n",
    "                          attention_mask=attention_mask,\n",
    "                          token_type_ids=token_type_ids)\n",
    "      \n",
    "      # 실제로 필요한 것은 CLS 에 해당하는 embedding 이기 때문에, 이만 출력\n",
    "      pooled_output = outputs[1]\n",
    "\n",
    "      return pooled_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 24450,
     "status": "ok",
     "timestamp": 1616574119714,
     "user": {
      "displayName": "Miyoung Ko",
      "photoUrl": "",
      "userId": "12757166311753740653"
     },
     "user_tz": -540
    },
    "id": "wnO1b30SomBP",
    "outputId": "998e38f2-0564-4956-bd43-878798158805"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertEncoder: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertEncoder from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertEncoder from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertEncoder: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertEncoder from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertEncoder from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# load pre-trained model on cuda (if available)\n",
    "p_encoder = BertEncoder.from_pretrained(model_checkpoint)\n",
    "q_encoder = BertEncoder.from_pretrained(model_checkpoint)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "  p_encoder.cuda()\n",
    "  q_encoder.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f3Dgo8U997HD"
   },
   "source": [
    "Train function 정의 후, 두개의 encoder fine-tuning 하기 (In-batch negative 활용) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "VAb7NpUc8YRo"
   },
   "outputs": [],
   "source": [
    "def train(args, dataset, p_model, q_model):\n",
    "  \n",
    "  # Dataloader\n",
    "  train_sampler = RandomSampler(dataset)\n",
    "  train_dataloader = DataLoader(dataset, sampler=train_sampler, batch_size=args.per_device_train_batch_size)\n",
    "\n",
    "  # optimizer\n",
    "  no_decay = ['bias', 'LayerNorm.weight']\n",
    "  optimizer_grouped_parameters = [\n",
    "        {'params': [p for n, p in p_model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': args.weight_decay},\n",
    "        {'params': [p for n, p in p_model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0},\n",
    "        {'params': [p for n, p in q_model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': args.weight_decay},\n",
    "        {'params': [p for n, p in q_model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "        ]\n",
    "\n",
    "  optimizer = AdamW(optimizer_grouped_parameters, lr = args.learning_rate, eps = args.adam_epsilon)\n",
    "  t_total = len(train_dataloader) // args.gradient_accumulation_steps * args.num_train_epochs\n",
    "  scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=args.warmup_steps, num_training_steps=t_total)\n",
    "\n",
    "  # Start training!\n",
    "  global_step = 0\n",
    "  \n",
    "  p_model.zero_grad()\n",
    "  q_model.zero_grad()\n",
    "  torch.cuda.empty_cache()\n",
    "  \n",
    "  train_iterator = trange(int(args.num_train_epochs), desc=\"Epoch\")\n",
    "\n",
    "  for _ in train_iterator:\n",
    "    epoch_iterator = tqdm(train_dataloader, desc=\"Iteration\")\n",
    "\n",
    "    for step, batch in enumerate(epoch_iterator):\n",
    "      q_encoder.train()\n",
    "      p_encoder.train()\n",
    "      \n",
    "      if torch.cuda.is_available():\n",
    "        batch = tuple(t.cuda() for t in batch)\n",
    "\n",
    "      p_inputs = {'input_ids': batch[0],\n",
    "                  'attention_mask': batch[1],\n",
    "                  'token_type_ids': batch[2]\n",
    "                  }\n",
    "      \n",
    "      q_inputs = {'input_ids': batch[3],\n",
    "                  'attention_mask': batch[4],\n",
    "                  'token_type_ids': batch[5]}\n",
    "      \n",
    "      p_outputs = p_model(**p_inputs)  # (batch_size, emb_dim)\n",
    "      q_outputs = q_model(**q_inputs)  # (batch_size, emb_dim)\n",
    "\n",
    "\n",
    "      # Calculate similarity score & loss\n",
    "      sim_scores = torch.matmul(q_outputs, torch.transpose(p_outputs, 0, 1))  # (batch_size, emb_dim) x (emb_dim, batch_size) = (batch_size, batch_size)\n",
    "\n",
    "      # target: position of positive samples = diagonal element \n",
    "      targets = torch.arange(0, args.per_device_train_batch_size).long()\n",
    "      if torch.cuda.is_available():\n",
    "        targets = targets.to('cuda')\n",
    "\n",
    "      sim_scores = F.log_softmax(sim_scores, dim=1)\n",
    "\n",
    "      loss = F.nll_loss(sim_scores, targets)\n",
    "      print(loss)\n",
    "\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "      scheduler.step()\n",
    "      q_model.zero_grad()\n",
    "      p_model.zero_grad()\n",
    "      global_step += 1\n",
    "      \n",
    "      torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "    \n",
    "  return p_model, q_model\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "ICSJoJrUDGZ5"
   },
   "outputs": [],
   "source": [
    "args = TrainingArguments(\n",
    "    output_dir=\"dense_retireval\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    num_train_epochs=2,\n",
    "    weight_decay=0.01\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 96513,
     "status": "ok",
     "timestamp": 1616574191784,
     "user": {
      "displayName": "Miyoung Ko",
      "photoUrl": "",
      "userId": "12757166311753740653"
     },
     "user_tz": -540
    },
    "id": "E8a7ww3WgsaZ",
    "outputId": "8f98f6cf-8a44-4e4a-928b-836a1b27a772"
   },
   "outputs": [],
   "source": [
    "p_encoder, q_encoder = train(args, train_dataset, p_encoder, q_encoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BGOw-k7Ln85t"
   },
   "source": [
    "## Dense Embedding을 활용하여 passage retrieval 실습해보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 96984,
     "status": "ok",
     "timestamp": 1616574192258,
     "user": {
      "displayName": "Miyoung Ko",
      "photoUrl": "",
      "userId": "12757166311753740653"
     },
     "user_tz": -540
    },
    "id": "NouB9uBcTaws",
    "outputId": "f2446b6c-3008-4350-be54-140b194a1a07"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "유아인에게 타고난 배우라고 말한 드라마 밀회의 감독은?\n",
      "화보 촬영을 위해 미국에 있을 때, 김희애의 연락을 통해 JTBC 드라마 《밀회》의 캐스팅을 제안받았다. 당시 영화 《베테랑》에 이미 캐스팅된 상태였으나, 유아인은 류승완 감독과 제작사의 양해를 얻어 《밀회》에 출연한다. 천재 피아니스트 ‘이선재’ 역할을 위해 피아니스트들의 영상을 보고 곡의 스피드와 건반 위치 등을 외워 실제 타건을 하며 촬영했다. 피아노 울림판을 수건으로 막고 타건을 하면, 그 후 대역 피아니스트의 소리를 덧입히는 방식이었다. 《밀회》는 작품성을 인정받고 숱한 화제를 낳으며 당시 종편으로서는 높은 시청률을 기록했다. 유아인은 섬세한 연기력을 선보여 순수함으로 시청자들을 매료시켰다는 호평을 얻었고, 특히 피아노 연주에 있어서 클래식 종사자들에게 인정을 받았다. 연출을 맡은 안판석 감독은 유아인에 대해 “느낌으로만 연기를 하는 게 아니고 감성을 지적으로 통제해 가면서 연기한다. 그 나이에”라며 “타고난 배우”라고 말했다. 유아인은 《밀회》를 통해 예술적인 면모를 구체화할 수 있어서 만족감을 느꼈다고 밝혔으며, 종영 후 자신의 페이스북 계정에 긴 소감글을 남겼다. 특히 ‘이선재’ 캐릭터를 배우 유아인이 가진 소년성의 엑기스로 생각하며, 2015년 10월 부산국제영화제 오픈토크에서는 본인이 가장 좋아하는 캐릭터로 꼽았다. \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "valid_corpus = list(set([example['context'] for example in dataset['validation']]))[:10]\n",
    "sample_idx = random.choice(range(len(dataset['validation'])))\n",
    "query = dataset['validation'][sample_idx]['question']\n",
    "ground_truth = dataset['validation'][sample_idx]['context']\n",
    "\n",
    "if not ground_truth in valid_corpus:\n",
    "  valid_corpus.append(ground_truth)\n",
    "\n",
    "print(query)\n",
    "print(ground_truth, '\\n\\n')\n",
    "\n",
    "# valid_corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "05D8GzFrJhHO"
   },
   "source": [
    "앞서 학습한 passage encoder, question encoder을 이용해 dense embedding 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "ba-hH3NQOEWJ"
   },
   "outputs": [],
   "source": [
    "def to_cuda(batch):\n",
    "  return tuple(t.cuda() for t in batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 97547,
     "status": "ok",
     "timestamp": 1616574192826,
     "user": {
      "displayName": "Miyoung Ko",
      "photoUrl": "",
      "userId": "12757166311753740653"
     },
     "user_tz": -540
    },
    "id": "YufA_ayPJBRg",
    "outputId": "fa600009-393a-4f93-871e-bff93675d05e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([11, 768]) torch.Size([1, 768])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "  p_encoder.eval()\n",
    "  q_encoder.eval()\n",
    "\n",
    "  q_seqs_val = tokenizer([query], padding=\"max_length\", truncation=True, return_tensors='pt').to('cuda')\n",
    "  q_emb = q_encoder(**q_seqs_val).to('cpu')  #(num_query, emb_dim)\n",
    "\n",
    "  p_embs = []\n",
    "  for p in valid_corpus:\n",
    "    p = tokenizer(p, padding=\"max_length\", truncation=True, return_tensors='pt').to('cuda')\n",
    "    p_emb = p_encoder(**p).to('cpu').numpy()\n",
    "    p_embs.append(p_emb)\n",
    "\n",
    "p_embs = torch.Tensor(p_embs).squeeze()  # (num_passage, emb_dim)\n",
    "\n",
    "print(p_embs.size(), q_emb.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pOHHak7WS1ko"
   },
   "source": [
    "생성된 embedding에 dot product를 수행 => Document들의 similarity ranking을 구함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 97546,
     "status": "ok",
     "timestamp": 1616574192827,
     "user": {
      "displayName": "Miyoung Ko",
      "photoUrl": "",
      "userId": "12757166311753740653"
     },
     "user_tz": -540
    },
    "id": "xn5Cx5JkKZJB",
    "outputId": "eb232f4f-bdae-474d-b630-a13d0b563364"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 11])\n",
      "tensor([[24.0346, 24.3444, 24.1337, 24.3273, 24.8338, 24.5397, 24.5433, 24.3544,\n",
      "         24.0145, 24.3972, 24.9225]])\n",
      "tensor([10,  4,  6,  5,  9,  7,  1,  3,  2,  0,  8])\n"
     ]
    }
   ],
   "source": [
    "dot_prod_scores = torch.matmul(q_emb, torch.transpose(p_embs, 0, 1))\n",
    "print(dot_prod_scores.size())\n",
    "\n",
    "rank = torch.argsort(dot_prod_scores, dim=1, descending=True).squeeze()\n",
    "print(dot_prod_scores)\n",
    "print(rank)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Oq2Oiv8MKVS6"
   },
   "source": [
    "Top-5개의 passage를 retrieve 하고 ground truth와 비교하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 97544,
     "status": "ok",
     "timestamp": 1616574192827,
     "user": {
      "displayName": "Miyoung Ko",
      "photoUrl": "",
      "userId": "12757166311753740653"
     },
     "user_tz": -540
    },
    "id": "WaStRXYdJ-wI",
    "outputId": "bbf42d40-dfae-49ff-bf12-139452b5849f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Search query]\n",
      " 유아인에게 타고난 배우라고 말한 드라마 밀회의 감독은? \n",
      "\n",
      "[Ground truth passage]\n",
      "화보 촬영을 위해 미국에 있을 때, 김희애의 연락을 통해 JTBC 드라마 《밀회》의 캐스팅을 제안받았다. 당시 영화 《베테랑》에 이미 캐스팅된 상태였으나, 유아인은 류승완 감독과 제작사의 양해를 얻어 《밀회》에 출연한다. 천재 피아니스트 ‘이선재’ 역할을 위해 피아니스트들의 영상을 보고 곡의 스피드와 건반 위치 등을 외워 실제 타건을 하며 촬영했다. 피아노 울림판을 수건으로 막고 타건을 하면, 그 후 대역 피아니스트의 소리를 덧입히는 방식이었다. 《밀회》는 작품성을 인정받고 숱한 화제를 낳으며 당시 종편으로서는 높은 시청률을 기록했다. 유아인은 섬세한 연기력을 선보여 순수함으로 시청자들을 매료시켰다는 호평을 얻었고, 특히 피아노 연주에 있어서 클래식 종사자들에게 인정을 받았다. 연출을 맡은 안판석 감독은 유아인에 대해 “느낌으로만 연기를 하는 게 아니고 감성을 지적으로 통제해 가면서 연기한다. 그 나이에”라며 “타고난 배우”라고 말했다. 유아인은 《밀회》를 통해 예술적인 면모를 구체화할 수 있어서 만족감을 느꼈다고 밝혔으며, 종영 후 자신의 페이스북 계정에 긴 소감글을 남겼다. 특히 ‘이선재’ 캐릭터를 배우 유아인이 가진 소년성의 엑기스로 생각하며, 2015년 10월 부산국제영화제 오픈토크에서는 본인이 가장 좋아하는 캐릭터로 꼽았다. \n",
      "\n",
      "Top-1 passage with score 24.9225\n",
      "화보 촬영을 위해 미국에 있을 때, 김희애의 연락을 통해 JTBC 드라마 《밀회》의 캐스팅을 제안받았다. 당시 영화 《베테랑》에 이미 캐스팅된 상태였으나, 유아인은 류승완 감독과 제작사의 양해를 얻어 《밀회》에 출연한다. 천재 피아니스트 ‘이선재’ 역할을 위해 피아니스트들의 영상을 보고 곡의 스피드와 건반 위치 등을 외워 실제 타건을 하며 촬영했다. 피아노 울림판을 수건으로 막고 타건을 하면, 그 후 대역 피아니스트의 소리를 덧입히는 방식이었다. 《밀회》는 작품성을 인정받고 숱한 화제를 낳으며 당시 종편으로서는 높은 시청률을 기록했다. 유아인은 섬세한 연기력을 선보여 순수함으로 시청자들을 매료시켰다는 호평을 얻었고, 특히 피아노 연주에 있어서 클래식 종사자들에게 인정을 받았다. 연출을 맡은 안판석 감독은 유아인에 대해 “느낌으로만 연기를 하는 게 아니고 감성을 지적으로 통제해 가면서 연기한다. 그 나이에”라며 “타고난 배우”라고 말했다. 유아인은 《밀회》를 통해 예술적인 면모를 구체화할 수 있어서 만족감을 느꼈다고 밝혔으며, 종영 후 자신의 페이스북 계정에 긴 소감글을 남겼다. 특히 ‘이선재’ 캐릭터를 배우 유아인이 가진 소년성의 엑기스로 생각하며, 2015년 10월 부산국제영화제 오픈토크에서는 본인이 가장 좋아하는 캐릭터로 꼽았다.\n",
      "Top-2 passage with score 24.8338\n",
      "1992년 3월에는 노동조합 간부에 대해 제기된 국가보안법 7조5항(이적표현물 제작·소지·배포 위반 상고심에서 \"‘결과적으로’ 반국가단체를 이롭게 할 수도 있다는 ‘미필적 인식’만 있으면 처벌할 수 있다\"는 다수의견과 달리 “이같은 판단기준은 추상적이고 애매모호하여 어느 정도의 표현이 법에 의하여 규제되고 처벌대상이 되는지 예측할 수 없게 만든다”며 “불법성의 판단기준이 구체적으로 밝혀져야 한다”고 하면서 “피의자의 고의성만을 판단해 처벌하게 되면 헌법상 보장된 사상과 양심의 자유를 위협하는 요소로 작용할 수 있다, 유해한 사상과 이념이라 할지라도 가급적 자유경쟁의 시장에서 비판되고 도태되는 과정을 거치게 해야 한다. 이적행위의 요건이 전혀 특정되지 않아 구성요건적 행위의 정형성이 결여됨으로써 죄형의 명확성을 요구하는 죄형법정주의에 반한다. 북한의 선전내용을 찬양·고무하는 행위라도 대한민국의 존립을 파괴할 구체적이고 가능한 위험이 있는 행위라고 보기 어려울 때는 불법성을 부인할 수밖에 없다\"며 법 운용 상의 위헌요소와 인권침해 가능성을 지적하며 법 적용범위를 엄격히 할 것을 주장하는 소수의견을 냈다. 이는 1995년 사회민주주의청년연맹 사건으로 구속기소된 최형록이나 전국연합 상임의장 이창복 사건 등에 있어 소장판사들이 일부 국가보안법 위반사범에 대해 무죄판결을 내리는 근거가 되어 사법부 내에서 국가보안법 위헌논쟁을 불러왔다. 하지만 정계 입문하고 나서 김대중 정부에서 대통령이 8.15경축사에서 밝힌 국가보안법 개정방침에 대해 강력하게 반대했다.\n",
      "Top-3 passage with score 24.5433\n",
      "전 신민당 총재 이민우는 말년에 김영삼이 이회창을 공천해 놓고는 자기 아들(김현철)을 2인자로 만들려고 이인제를 밀었다며 정치하는 사람이 그럴 수는 없는 법이며, 정치 도의도 없고, 신의도 없다며 비난하였다. 그 외에 새정치국민회의 정한용 의원 등으로부터 비자금 1000억원 조성 의혹이 제기되었으나 증거불충분으로 곧 묻혀졌다. 그 외에 \"YS는 오랜 세월 야당지도자로 있으면서 민심을 잘 읽는 정치를 해왔다. YS는 대통령이 된 뒤 안기부의 정치활동 간여 금지를 골자로 한 안기부법 개정을 단행하는 등 정보정치와 확실한 거리를 두는 듯했다. 하지만 1년쯤 지나면서 안기부 정보보고서를 찾는 빈도가 늘어났다. 마약과도 같다는 정보에 중독되기 시작한 것이다. 그러면서 점점 민심의 추이에는 둔감해졌다.\"는 비판도 있다. 한편 군사정권 출신과 보수 세력은 그가 좌파의 숙주가 되었다고 비판하기도 한다.\n",
      "Top-4 passage with score 24.5397\n",
      "정의로 구분짓자면, 블루스는 음악적 형태와, 장르, 재즈는 음악적 예술 형태이다. 블루스는 코드 진행과 장르를 이 안에서 만들었다고 확신할 수 있지만, 재즈는 이것을 파악하기 매우 어렵다. 19세기의 렉타임과 모던 퓨전 음악을 전부 포함하고 있어 범위가 매우 넓기 때문이다. 블루스와 재즈는 이러한 점에서 차이점들이 있지만, 사실 공통점이 많다. 둘다 미국 남부 (Deep South)와 19세기 말에 만들어졌다. 블루스는 아프리카-아메리칸 그들의 노동요, 스피릿튜얼스, 필드 찬트 그리고 할러에서 왔다. 또한 특징인 코드 진행, 플랫, 혹은 밴드 노트, 블루 노트, 그리고 슬프고 우울한 가사가 있다. 재즈도 마찬가지로 남부 아프리카계 미국인으로부터 나왔지만, 결과적으로 아프리카와 유럽 음악이 섞였다. 재즈 초기 당시 유명한 음악으로 인정받았고, 특색으로 블루 노트, 즉흥연주, 싱코페이션(당김음, 흔히들 싱코라고 하는 이것은 어울리는 한도 안에서 박자를 수정하는 것을 말한다), 그리고 '스윙 노트'를 알렸다. 재즈는 초기 뉴올리언스 딕시랜드, 스윙 시대의 빅 밴드 음악, 비밥, 라틴 재즈, 퓨전, 에시드 재즈, 펑크, 힙합, 그리고 당연히 블루스에게도 영향을 미쳤다.\n",
      "Top-5 passage with score 24.3972\n",
      "대한민국 해군은 27일 오전 수상함 10여 척과 해난구조함 평택함을 포함한 대부분의 병력을 사건 지점에 배치했다. 사건 지점에 배치된 100여 명의 해군 해난구조대(SSU) 잠수 요원들은 선체의 구멍을 조사했다. 28일에는 실종자 수색을 위해 광양함을 추가로 배치했다. 수색 과정에서 대한민국 해군은 침몰 지점으로부터 서남방 16마일 부근에서 구명복 상의 22개와 안전모 15개를 발견했다. 해군 SSU, UDT 대원들과 육군 특전사의 잠수요원들이 수색,구조작업에 투입되었다. 29일 밤, 아시아 최대의 수송함인 독도함을 침몰한 천안함의 탐색 및 구조 활동을 지휘하기 위해 현장 해역에 긴급 투입했다. 31일 함수쪽을 수색한 잠수사는, 격실 안에 물이 가득 차 있다고 증언했다.\n"
     ]
    }
   ],
   "source": [
    "k = 5\n",
    "print(\"[Search query]\\n\", query, \"\\n\")\n",
    "print(\"[Ground truth passage]\")\n",
    "print(ground_truth, \"\\n\")\n",
    "\n",
    "for i in range(k):\n",
    "  print(\"Top-%d passage with score %.4f\" % (i+1, dot_prod_scores.squeeze()[rank[i]]))\n",
    "  print(valid_corpus[rank[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lBaKYpdoXDcW"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "MRC Practice 5 - Dense Passage Retrieval (In-batch)",
   "provenance": [
    {
     "file_id": "1c9Vr7z_LBG2l9K4lVb40pu7Kk22hXQCp",
     "timestamp": 1614240569955
    },
    {
     "file_id": "1Q7iAXm_kwF_NHfOEGdViMCiPHnqoZlXe",
     "timestamp": 1613491158162
    }
   ]
  },
  "interpreter": {
   "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
